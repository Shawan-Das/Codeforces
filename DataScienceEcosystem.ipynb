{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Science Tools and Ecosystem"
      ],
      "metadata": {
        "id": "JDmG2qVH-4Nq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Science Tools and Ecosystem"
      ],
      "metadata": {
        "id": "jYVLfkXE---A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# List of Data Science Languages:\n",
        "\n"
      ],
      "metadata": {
        "id": "2XRMNos2_YG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Python\n",
        "2.   R\n",
        "3.   SQL\n",
        "4. Julia\n",
        "5. Scala\n",
        "6. Java\n",
        "7. Matlab"
      ],
      "metadata": {
        "id": "qgL0PmskAlgA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  list of data science libraries"
      ],
      "metadata": {
        "id": "FXJ4y-f_Av-I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Python Libraries\n",
        "1. NumPy\n",
        "2. Pandas\n",
        "3. Matplotlib\n",
        "4. Seaborn\n",
        "5. Scikit-learn\n",
        "6. TesorFlow\n",
        "7. Pytorch\n",
        "8. Keras\n",
        "9. Statsmodels\n",
        "10. NLTK\n",
        "\n",
        "R Libraries\n",
        "1. ggplot2\n",
        "2. dplyr\n",
        "3. tidyr\n",
        "4. caret\n",
        "5. glmnet\n",
        "6. lubridate\n",
        "7. tm"
      ],
      "metadata": {
        "id": "_8bEZb-8Ayt4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Here's a table outlining various data science tools across different categories:"
      ],
      "metadata": {
        "id": "uPHHE8sIDfH3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Category             | Tool                            | Description                                                                                   |\n",
        "|----------------------|---------------------------------|-----------------------------------------------------------------------------------------------|\n",
        "| Programming Languages| Python                          | General-purpose language with rich libraries for data manipulation, analysis, and machine learning.|\n",
        "|                      | R                               | Statistical computing and graphics language with extensive packages for data analysis and visualization.|\n",
        "|                      | SQL                             | Standardized language for managing and manipulating relational databases.                      |\n",
        "|                      | Julia                           | High-level, high-performance language for numerical computing and scientific applications.    |\n",
        "|                      | Scala                           | Scalable language used in big data processing frameworks like Apache Spark.                    |\n",
        "| Data Manipulation    | Pandas (Python)                 | Data manipulation and analysis library for Python, providing data structures and operations.  |\n",
        "|                      | dplyr (R)                       | Data manipulation package in R, facilitating data wrangling tasks with a concise syntax.      |\n",
        "| Data Visualization   | Matplotlib (Python)             | Comprehensive plotting library for creating static, interactive, and animated visualizations in Python.|\n",
        "|                      | ggplot2 (R)                     | Grammar of Graphics implementation in R, enabling creation of customized visualizations.      |\n",
        "|                      | Seaborn (Python)                | Statistical data visualization library based on Matplotlib, offering additional high-level interface.|\n",
        "| Machine Learning     | Scikit-learn (Python)           | Simple and efficient tools for data mining and data analysis, including various machine learning algorithms.|\n",
        "|                      | TensorFlow (Python)             | Open-source machine learning framework developed by Google for building and training deep learning models.|\n",
        "|                      | PyTorch (Python)                | Deep learning library developed by Facebook's AI Research lab, known for flexibility and dynamic computational graph.|\n",
        "|                      | caret (R)                       | Package for training and evaluating predictive models in R, streamlining the machine learning workflow.|\n",
        "| Natural Language Processing | NLTK (Python)               | Suite of libraries and programs for symbolic and statistical natural language processing in Python.|\n",
        "|                      | tm (R)                          | Text mining package in R, providing functions for creating, manipulating, and analyzing text documents.|\n",
        "| Big Data Processing  | Apache Spark                    | Distributed computing system for large-scale data processing, offering APIs in Scala, Java, and Python.|\n",
        "|                      | Hadoop                          | Framework for distributed storage and processing of large datasets across clusters of computers.|\n",
        "| Cloud Platforms      | AWS                             | Amazon Web Services cloud platform offering various services for data storage, processing, and analysis.|\n",
        "|                      | Azure                           | Microsoft's cloud computing platform with services for building, deploying, and managing applications and services.|\n",
        "|                      | Google Cloud Platform (GCP)     | Suite of cloud computing services by Google, including data storage, analytics, machine learning, and more.|\n",
        "| Integrated Development Environments (IDEs) | Jupyter Notebooks     | Web-based interactive computing environment for creating and sharing documents containing live code, equations, visualizations, and narrative text.|\n",
        "|                      | RStudio                         | Integrated development environment for R, providing tools for code editing, debugging, and visualization.|\n",
        "|                      | PyCharm                         | Python IDE with advanced features for code analysis, debugging, and integrated testing.           |"
      ],
      "metadata": {
        "id": "hUV9RqEBBoF5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#arithmetic expression examples"
      ],
      "metadata": {
        "id": "5zaDGAAoDiRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "5+20"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qu771SR-Dt0R",
        "outputId": "f2e03e44-5200-4747-927b-730764f0f766"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(5*4)+20"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4u2MT2Y6D53N",
        "outputId": "a75a417d-df45-454c-9f38-590fe21a483f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = int(input())\n",
        "print(f\"{m//60} hrs {m%60} Min\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2phf3a0iEBsf",
        "outputId": "0586a6f9-ee4b-4aa6-b44f-eca0ac8384f7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "130\n",
            "2 hrs 10 Min\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Objectives"
      ],
      "metadata": {
        "id": "bPae0upNFNgw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Data Collection**: Gather relevant data from various sources, including databases, APIs, files, and streaming platforms.\n",
        "\n",
        "2. **Data Cleaning and Preprocessing**: Prepare the data by cleaning, transforming, and structuring it to ensure accuracy and consistency.\n",
        "\n",
        "3. **Exploratory Data Analysis (EDA)**: Understand the underlying patterns, distributions, and relationships within the data through visualization and statistical analysis.\n",
        "\n",
        "4. **Feature Engineering**: Create new features or transform existing ones to enhance the predictive power of machine learning models.\n",
        "\n",
        "5. **Model Development**: Build predictive models using machine learning algorithms or statistical techniques to address specific business or research questions.\n",
        "\n",
        "6. **Model Evaluation and Validation**: Assess the performance of models using appropriate metrics and validation techniques to ensure their reliability and generalizability.\n",
        "\n",
        "7. **Model Deployment**: Implement models into production environments, making them accessible for real-time inference or decision-making.\n",
        "\n",
        "8. **Continuous Monitoring and Improvement**: Monitor model performance over time and iterate on models to adapt to changing data patterns and business requirements.\n",
        "\n",
        "9. **Insights Generation**: Extract meaningful insights and actionable recommendations from data analysis to support decision-making and strategic planning.\n",
        "\n",
        "10. **Predictive Analytics**: Utilize predictive models to forecast future trends, identify potential risks, and optimize resource allocation.\n",
        "\n",
        "11. **Anomaly Detection**: Identify unusual patterns or outliers in data that may indicate fraud, errors, or other irregularities.\n",
        "\n",
        "12. **Optimization and Automation**: Develop algorithms and systems to automate repetitive tasks, optimize processes, and enhance efficiency.\n",
        "\n",
        "13. **Data Governance and Security**: Establish policies, processes, and controls to ensure data quality, integrity, privacy, and compliance with regulatory requirements.\n",
        "\n",
        "14. **Collaboration and Communication**: Effectively communicate findings, insights, and recommendations to stakeholders from diverse backgrounds, including non-technical audiences."
      ],
      "metadata": {
        "id": "xI2sZEoMFPjw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Author’s name"
      ],
      "metadata": {
        "id": "u35SUcCRFUKg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Shawan Das**\n",
        "\n",
        "*University of Asia Pacific*\n",
        "\n",
        "contact: mailme.shawandas@gmail.com"
      ],
      "metadata": {
        "id": "-NVzjMpoFbu3"
      }
    }
  ]
}